%This document will explain the major features of the document
%class. For further information, the {\itshape \LaTeX\ User's Guide} is
%available from
%\url{https://www.acm.org/publications/proceedings-template}.
\documentclass[sigconf]{acmart}

\usepackage{lipsum}

\begin{document}

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
\settopmatter{printfolios=true}


\title{The Name of the Title is Hope}
\author{Your Name}
\affiliation{\institution{University of Passau}}
\email{your.name@uni-passau.de}




\begin{abstract}
 Here goes your abstract.
\end{abstract}



%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
%\keywords{datasets, neural networks, gaze detection, text tagging}




\maketitle

\section{Introduction}
Adaptive learning algorithms like AdaGrad and Adam are being extensively used thanks to there shorter learning time, However the benefit of using such algorithms on datasets where the number of model parameters to train on is larger than the available data, is to be investigated.
In this paper we try to reproduce the experiment done by Wilson \& al.~\cite{wilson2017marginal} in order to investigate the results of using adaptive learning algorithms compared to there non-adaptive counter parts in the case where the available number of points in the data is smaller than the number of model parameters to train.



\section{Background}
In this section we are going to introduce the algorithms used in the experiment. We are going to introduce them using an informal and a mathematical description. To visualize how these algorithms function differently we suggest that you take a look at Lili Jiang's tool Gradient Descent Viz \cite{lilipads}.\newline
In the next sections we define $x$ to be a vector or a scalar, $t$ is the iteration number, $L$ is the loss function, $W$ is the weights matrix, $\alpha$ is the learning rate and $\beta_i$ are decay rates. 
\subsection{Non-adaptive algorithms}
\textbf{Stochastic Gradient descent}~\cite{kiefer1952} is the most basic Learning algorithm. To minimize the Loss function it calculates the function's gradient at a particular point and updates the point coordinates with the negative value of that gradient.
Formally, in the iteration t SGD calculates the next point coordinate using the following formula:
\begin{center}
    $x_{t+1} = x_t -\alpha * \nabla L(x_t)$
\end{center}
One problem with SGD is that its learning speed is very slow~\cite{SGDSlow} and can get caught in a local minimum easily~\cite{SGDLocalMinimum}.



\subsection{Part Two}

\lipsum[3-4]

\section{Part Three}

\lipsum[5-6]

\section{Related Work}

\section{Conclusion}

\section{Modifications}






\bibliographystyle{alpha}
\bibliography{literature}


\appendix


\end{document}
\endinput

